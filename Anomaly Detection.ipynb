{"metadata":{"colab":{"name":"Anomalyv6","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8246059,"sourceType":"datasetVersion","datasetId":4892246},{"sourceId":8261740,"sourceType":"datasetVersion","datasetId":4903746}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'logsanaly:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4892246%2F8246059%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240430%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240430T135626Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D04e0a1026314ff8c04c7756042091ed669b751ab7b53ca9435bd33636a1f3030311f7fce2179b2206b9918d18e309ce4fb5d57a93b39902c190703d56c6a708d77338061568fa25ca3a0def447f06e64f2555762cd60c8a2f44f708c741208e7aca30f5e6df16dd08e91de281bfa29d8c1439aaa32d1a5adbbc622dfac42143d04df9bd2f08267882e4883ab6dcd5df5dc9d08de9f8ce8407a52ebc9b9b2ff4f1f7838bf347e267dba64585113d3b6273149f711c6fcede67b430b3550a675d80fa1c8b9afe3ab93e02ab44638138ef38afc89339902097d891abd8b491be20ac4d97866f08dead7c07d800748bc8807ce5908cefdee47bb2d0ef49bb5f013e8,nudatasetkevinnormal:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4903746%2F8261740%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240430%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240430T135626Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1399b630f1f734cf9ddc6578e9efb187115cd4cb86e58844aecbcf458a32cef98f2c86a1d80adbc1d6ac9e08b975b64065872d94538019e09b9591cc76cfb75fd52f90bfac4000977bc030a2f2f642e5726d73f23314e5800b7db449044c0dd7011367622e93919aa76ea339158a813e558298a1f3f270b38e8516a2cd35a7f679ff61fe3272b0e64d83b6f720dff1fd027ff033dcfbecc52323c1ad6f5f0e627c6a4e79c5f3a8a2e280d8bbf9bf099a48db816be6a8bb875e0dfaf0920b0c21fe0346cfe4f77e7d948c7c19a5a21bfece780d7bae428c4a3ffec7acbe61dc2e098e09be3b83a6248a14502feca2db20364dbe1b3e37df075882de4812e1ffff'\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{"id":"-hzdC6HzSuYg","execution":{"iopub.status.busy":"2024-05-21T12:26:46.116389Z","iopub.execute_input":"2024-05-21T12:26:46.117061Z","iopub.status.idle":"2024-05-21T12:26:47.244426Z","shell.execute_reply.started":"2024-05-21T12:26:46.117031Z","shell.execute_reply":"2024-05-21T12:26:47.243254Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/4892246/8246059/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240430%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240430T135626Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=04e0a1026314ff8c04c7756042091ed669b751ab7b53ca9435bd33636a1f3030311f7fce2179b2206b9918d18e309ce4fb5d57a93b39902c190703d56c6a708d77338061568fa25ca3a0def447f06e64f2555762cd60c8a2f44f708c741208e7aca30f5e6df16dd08e91de281bfa29d8c1439aaa32d1a5adbbc622dfac42143d04df9bd2f08267882e4883ab6dcd5df5dc9d08de9f8ce8407a52ebc9b9b2ff4f1f7838bf347e267dba64585113d3b6273149f711c6fcede67b430b3550a675d80fa1c8b9afe3ab93e02ab44638138ef38afc89339902097d891abd8b491be20ac4d97866f08dead7c07d800748bc8807ce5908cefdee47bb2d0ef49bb5f013e8 to path /kaggle/input/logsanaly\nFailed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/4903746/8261740/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240430%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240430T135626Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1399b630f1f734cf9ddc6578e9efb187115cd4cb86e58844aecbcf458a32cef98f2c86a1d80adbc1d6ac9e08b975b64065872d94538019e09b9591cc76cfb75fd52f90bfac4000977bc030a2f2f642e5726d73f23314e5800b7db449044c0dd7011367622e93919aa76ea339158a813e558298a1f3f270b38e8516a2cd35a7f679ff61fe3272b0e64d83b6f720dff1fd027ff033dcfbecc52323c1ad6f5f0e627c6a4e79c5f3a8a2e280d8bbf9bf099a48db816be6a8bb875e0dfaf0920b0c21fe0346cfe4f77e7d948c7c19a5a21bfece780d7bae428c4a3ffec7acbe61dc2e098e09be3b83a6248a14502feca2db20364dbe1b3e37df075882de4812e1ffff to path /kaggle/input/nudatasetkevinnormal\nData source import complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport dateutil\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import OneClassSVM\nfrom joblib import dump\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:26:47.246707Z","iopub.execute_input":"2024-05-21T12:26:47.247086Z","iopub.status.idle":"2024-05-21T12:26:52.672857Z","shell.execute_reply.started":"2024-05-21T12:26:47.247050Z","shell.execute_reply":"2024-05-21T12:26:52.671973Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Merging all logs together","metadata":{"id":"5yAsOrM1SuYh"}},{"cell_type":"code","source":"def read_logs_from_file(file_path):\n    with open(file_path, 'r') as file:\n        logs = file.readlines()\n    return logs","metadata":{"id":"g0hKwXRRSuYh","execution":{"iopub.status.busy":"2024-05-21T12:26:52.674531Z","iopub.execute_input":"2024-05-21T12:26:52.675097Z","iopub.status.idle":"2024-05-21T12:26:52.680349Z","shell.execute_reply.started":"2024-05-21T12:26:52.675062Z","shell.execute_reply":"2024-05-21T12:26:52.679014Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_logs_from_file(file_path):\n    with open(file_path, 'r') as file:\n        logs = file.readlines()\n    return logs\n# Step 1: Define the folder path\nfolder_path = '/kaggle/input/nudatasetkevinnormal/2024-04-29/wazuh-machine/logs/'\n\n# Step 2: Get a list of all CSV files in the folder\ncsv_files = [file for file in os.listdir(folder_path)]\n\n# Step 3: Read each CSV file into a pandas DataFrame and add filename column\ndfs = []\nfor file in csv_files:\n    logs = read_logs_from_file(os.path.join(folder_path, file))\n    df = pd.DataFrame(logs, columns=['log_entry'])\n    if file == 'dmesg':\n        file = 'dmesg.log'\n        print(file)\n    elif file == 'syslog':\n        file = 'syslog.log'\n        print(file)\n    elif file == 'wtmp':\n        file = 'wtmp.log'\n        print(file)\n\n    df['filename'] = file  # Add filename column\n    dfs.append(df)\n\n# Step 4: Concatenate all DataFrames into one\ncombined_df = pd.concat(dfs)\ncombined_df = combined_df.reset_index(drop=True)\n# Step 5: Save the combined DataFrame to a new CSV file\n#combined_df.to_csv('combined_data.csv', index=False)\n\nprint(\"Combining and appending filenames to rows complete.\")\nfor i, item in combined_df.iterrows():\n    file = combined_df.at[i, 'filename']\n    combined_df.at[i, 'log_entry'] = str(file) + \": \" + (str(combined_df.at[i, 'log_entry']))\n","metadata":{"id":"tIlpQ1LfSuYi","execution":{"iopub.status.busy":"2024-05-21T12:26:52.682316Z","iopub.execute_input":"2024-05-21T12:26:52.682616Z","iopub.status.idle":"2024-05-21T12:26:53.910607Z","shell.execute_reply.started":"2024-05-21T12:26:52.682595Z","shell.execute_reply":"2024-05-21T12:26:53.909850Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"syslog.log\nwtmp.log\ndmesg.log\nCombining and appending filenames to rows complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_logs_from_file(file_path):\n    with open(file_path, 'r') as file:\n        logs = file.readlines()\n    return logs\n# Step 1: Define the folder path\nfolder_path = '/kaggle/input/nudatasetkevinnormal/2024-04-29/coap-client/logs/'\n\n# Step 2: Get a list of all CSV files in the folder\ncsv_files = [file for file in os.listdir(folder_path)]\n\n# Step 3: Read each CSV file into a pandas DataFrame and add filename column\ndfs = []\nfor file in csv_files:\n    logs = read_logs_from_file(os.path.join(folder_path, file))\n    df = pd.DataFrame(logs, columns=['log_entry'])\n    if file == 'faillog':\n        file = 'faillog.log'\n        print(file)\n    elif file == 'syslog':\n        file = 'syslog.log'\n        print(file)\n    elif file == 'messages':\n        file = 'messages.log'\n        print(file)\n\n    df['filename'] = file  # Add filename column\n    dfs.append(df)\n\n# Step 4: Concatenate all DataFrames into one\ncombined_df2= pd.concat(dfs)\ncombined_df2 = combined_df2.reset_index(drop=True)\n\n# Step 5: Save the combined DataFrame to a new CSV file\n#combined_df.to_csv('combined_data.csv', index=False)\n\nprint(\"Combining and appending filenames to rows complete.\")\nfor i, item in combined_df2.iterrows():\n    file = combined_df2.at[i, 'filename']\n    combined_df2.at[i, 'log_entry'] = str(file) + \": \" + (str(combined_df2.at[i, 'log_entry']))\n","metadata":{"id":"G_bFkr9uSuYi","execution":{"iopub.status.busy":"2024-05-21T12:26:53.911697Z","iopub.execute_input":"2024-05-21T12:26:53.912046Z","iopub.status.idle":"2024-05-21T12:26:55.753524Z","shell.execute_reply.started":"2024-05-21T12:26:53.912014Z","shell.execute_reply":"2024-05-21T12:26:55.752588Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"syslog.log\nmessages.log\nfaillog.log\nCombining and appending filenames to rows complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"file_path3 = '/kaggle/input/logsanaly/UC753_final_-_CLEAR_TRAFFIC_-_NO_ATTACK.log' # normal traffic\n\nlogs3 = read_logs_from_file(file_path3)\ndf3 = pd.DataFrame(logs3, columns=['log_entry'])\n\ndf_merged = pd.concat([df3, combined_df, combined_df2], ignore_index=True, sort=False)\ndf = df_merged.copy(deep=True)\ndf = df.reset_index(drop=True)\ndf = df.drop('filename', axis=1)","metadata":{"id":"Mpcr__h7SuYj","execution":{"iopub.status.busy":"2024-05-21T12:26:55.754696Z","iopub.execute_input":"2024-05-21T12:26:55.754991Z","iopub.status.idle":"2024-05-21T12:26:55.808430Z","shell.execute_reply.started":"2024-05-21T12:26:55.754966Z","shell.execute_reply":"2024-05-21T12:26:55.807725Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Splitting filename and log content/entry\ncnt = 0\nfor i, item in df.iterrows():\n    try:\n        df.at[i, 'log_name'] = df['log_entry'][i].split(': ', 1)[0]\n        df.at[i, 'log_content'] = df['log_entry'][i].split(': ', 1)[1]\n    except:\n        df = df.drop(i)\n        cnt = cnt + 1\n","metadata":{"id":"C8qxIaFmSuYj","execution":{"iopub.status.busy":"2024-05-21T12:26:55.809422Z","iopub.execute_input":"2024-05-21T12:26:55.809660Z","iopub.status.idle":"2024-05-21T12:27:02.242806Z","shell.execute_reply.started":"2024-05-21T12:26:55.809638Z","shell.execute_reply":"2024-05-21T12:27:02.242065Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Function to extract and standardize timestamps\ndef extract_timestamp(log_entry):\n    try:\n        return dateutil.parser.parse(log_entry.split()[0])\n    except:\n        return None\n\ndf['timestamp'] = df['log_content'].apply(extract_timestamp)\nvectorizer = TfidfVectorizer(max_features=1000) # features tested from 10, 100 and None\nX_tfidf = vectorizer.fit_transform(df['log_content']).toarray()\ndf['log_file_cat'] = df['log_name'].astype('category').cat.codes\nX_combined = np.hstack((X_tfidf, df['log_file_cat'].values.reshape(-1, 1)))\n\nsequence_length = 10  # Defining the length of each sequence\nn_features = X_combined.shape[1]  # Number of features\n\n# This function creates sequences from the dataset\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length + 1):\n        x_seq = data[i:(i + seq_length)]\n        xs.append(x_seq)\n    return np.array(xs)\n\nX_seq = create_sequences(X_combined, sequence_length)\n\nX_train, X_test = train_test_split(X_seq, test_size=0.2, random_state=42, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T12:27:02.243831Z","iopub.execute_input":"2024-05-21T12:27:02.244106Z","iopub.status.idle":"2024-05-21T12:27:07.053027Z","shell.execute_reply.started":"2024-05-21T12:27:02.244082Z","shell.execute_reply":"2024-05-21T12:27:07.052136Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# OneClass SVM","metadata":{"id":"zHz-wh4ASuYm"}},{"cell_type":"code","source":"X_train, X_test = train_test_split(X_combined, test_size=0.2, random_state=42, shuffle=False)\n\n# Initialize One-Class SVM\noc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.05)  # nu is an upper bound on the fraction of training errors\n\n# Train the model on normal data\noc_svm.fit(X_train)\n\ndump(oc_svm, 'oc_svm.joblib')\n\n# Predict xusing the trained model\npredictions = oc_svm.predict(X_test)  # -1 for outliers (anomalies), 1 for inliers (normal)\narr = np.array(predictions)\nx = np.where(arr == -1)\nnumberOfAnol = [len(x) for x in x]\nprint(numberOfAnol) #","metadata":{"id":"9-SvacyKSuYn","execution":{"iopub.status.busy":"2024-05-21T12:27:07.054202Z","iopub.execute_input":"2024-05-21T12:27:07.054496Z","iopub.status.idle":"2024-05-21T12:28:55.104782Z","shell.execute_reply.started":"2024-05-21T12:27:07.054469Z","shell.execute_reply":"2024-05-21T12:28:55.103905Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[504]\n","output_type":"stream"}]},{"cell_type":"code","source":"# on attacks\nfile_path = '/kaggle/input/logsanaly/UC753_final_attack.log' # traffic with Attacks\nlogs = read_logs_from_file(file_path)\ndf = pd.DataFrame(logs, columns=['log_entry'])\n# Parse the log name and the content\ndf['log_name'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[0])\ndf['log_content'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[1])\ndf['timestamp'] = df['log_content'].apply(extract_timestamp)\nvectorizer = TfidfVectorizer(max_features=1000)\nX_tfidf = vectorizer.fit_transform(df['log_content']).toarray()\ndf['log_file_cat'] = df['log_name'].astype('category').cat.codes\nX_combined = np.hstack((X_tfidf, df['log_file_cat'].values.reshape(-1, 1)))\npredictions = oc_svm.predict(X_combined)  # -1 for outliers (anomalies), 1 for inliers (normal)\narr = np.array(predictions)\nx = np.where(arr == -1)\nnumberOfAnol = [len(x) for x in x]\nprint(numberOfAnol)\n\n\n# Find indices of anomalies (where predictions are -1)\nanomaly_indices = np.where(predictions == -1)[0]  # [0] to get the array from the tuple returned by np.where\n# Print the indices of anomalies\nprint(\"Indices of anomalies:\", anomaly_indices)\nprint(\"Number of anomalies:\", len(anomaly_indices))\nanomalous_entries = df.iloc[anomaly_indices]\n#print(anomalous_entries)\n","metadata":{"id":"oLdUmEr0SuYn","execution":{"iopub.status.busy":"2024-05-21T12:28:55.107460Z","iopub.execute_input":"2024-05-21T12:28:55.107715Z","iopub.status.idle":"2024-05-21T12:29:19.771352Z","shell.execute_reply.started":"2024-05-21T12:28:55.107693Z","shell.execute_reply":"2024-05-21T12:29:19.770429Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[3327]\nIndices of anomalies: [    0     6     7 ... 19489 19490 19491]\nNumber of anomalies: 3327\n","output_type":"stream"}]},{"cell_type":"code","source":"anomalous_entries['log_name'].value_counts()","metadata":{"id":"BJAPT8wJSuYn","execution":{"iopub.status.busy":"2024-05-21T12:29:19.772645Z","iopub.execute_input":"2024-05-21T12:29:19.773039Z","iopub.status.idle":"2024-05-21T12:29:19.784719Z","shell.execute_reply.started":"2024-05-21T12:29:19.773007Z","shell.execute_reply":"2024-05-21T12:29:19.783794Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"log_name\nbootstrap.log       2258\nauth.log             863\nalternatives.log     206\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# LSTM","metadata":{"id":"D8yrzJzISuYn"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTMAutoencoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(LSTMAutoencoder, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.decoder = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x):\n        _, (hidden, _) = self.lstm(x)\n        decoded = self.decoder(hidden.squeeze(0))\n        return decoded\n\n# Instantiate the model\ninput_size = n_features\nhidden_size = 256\nnum_layers = 2\n\nmodel = LSTMAutoencoder(input_size, hidden_size, num_layers).to('cuda')\nloss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"id":"dh6mebxGSuYn","execution":{"iopub.status.busy":"2024-05-21T13:17:41.820085Z","iopub.execute_input":"2024-05-21T13:17:41.820509Z","iopub.status.idle":"2024-05-21T13:17:41.853248Z","shell.execute_reply.started":"2024-05-21T13:17:41.820476Z","shell.execute_reply":"2024-05-21T13:17:41.852429Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:17:49.459507Z","iopub.execute_input":"2024-05-21T13:17:49.460346Z","iopub.status.idle":"2024-05-21T13:17:49.477333Z","shell.execute_reply.started":"2024-05-21T13:17:49.460309Z","shell.execute_reply":"2024-05-21T13:17:49.476226Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                               log_entry          log_name  \\\n0      bootstrap.log: 2023-05-01 21:34:47 URL:http://...     bootstrap.log   \n1      bootstrap.log: gpgv: Signature made Thu Apr 21...     bootstrap.log   \n2      bootstrap.log: gpgv:                using RSA ...     bootstrap.log   \n3      bootstrap.log: gpgv: Good signature from \"Ubun...     bootstrap.log   \n4      bootstrap.log: 2023-05-01 21:34:47 URL:http://...     bootstrap.log   \n...                                                  ...               ...   \n19487  alternatives.log: update-alternatives 2024-04-...  alternatives.log   \n19488  alternatives.log: update-alternatives 2024-04-...  alternatives.log   \n19489  alternatives.log: update-alternatives 2024-04-...  alternatives.log   \n19490  alternatives.log: update-alternatives 2024-04-...  alternatives.log   \n19491  alternatives.log: update-alternatives 2024-04-...  alternatives.log   \n\n                                             log_content            timestamp  \\\n0      2023-05-01 21:34:47 URL:http://ftpmaster.inter...  2023-05-01 00:00:00   \n1      gpgv: Signature made Thu Apr 21 17:16:39 2022 ...                 None   \n2      gpgv:                using RSA key 871920D1991...                 None   \n3      gpgv: Good signature from \"Ubuntu Archive Auto...                 None   \n4      2023-05-01 21:34:47 URL:http://ftpmaster.inter...  2023-05-01 00:00:00   \n...                                                  ...                  ...   \n19487  update-alternatives 2024-04-03 11:09:58: run w...                 None   \n19488  update-alternatives 2024-04-03 11:09:58: run w...                 None   \n19489  update-alternatives 2024-04-15 10:54:52: run w...                 None   \n19490  update-alternatives 2024-04-15 10:55:20: run w...                 None   \n19491  update-alternatives 2024-04-19 11:09:25: run w...                 None   \n\n       log_file_cat  \n0                 2  \n1                 2  \n2                 2  \n3                 2  \n4                 2  \n...             ...  \n19487             0  \n19488             0  \n19489             0  \n19490             0  \n19491             0  \n\n[19492 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>log_entry</th>\n      <th>log_name</th>\n      <th>log_content</th>\n      <th>timestamp</th>\n      <th>log_file_cat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bootstrap.log: 2023-05-01 21:34:47 URL:http://...</td>\n      <td>bootstrap.log</td>\n      <td>2023-05-01 21:34:47 URL:http://ftpmaster.inter...</td>\n      <td>2023-05-01 00:00:00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bootstrap.log: gpgv: Signature made Thu Apr 21...</td>\n      <td>bootstrap.log</td>\n      <td>gpgv: Signature made Thu Apr 21 17:16:39 2022 ...</td>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bootstrap.log: gpgv:                using RSA ...</td>\n      <td>bootstrap.log</td>\n      <td>gpgv:                using RSA key 871920D1991...</td>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bootstrap.log: gpgv: Good signature from \"Ubun...</td>\n      <td>bootstrap.log</td>\n      <td>gpgv: Good signature from \"Ubuntu Archive Auto...</td>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bootstrap.log: 2023-05-01 21:34:47 URL:http://...</td>\n      <td>bootstrap.log</td>\n      <td>2023-05-01 21:34:47 URL:http://ftpmaster.inter...</td>\n      <td>2023-05-01 00:00:00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19487</th>\n      <td>alternatives.log: update-alternatives 2024-04-...</td>\n      <td>alternatives.log</td>\n      <td>update-alternatives 2024-04-03 11:09:58: run w...</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19488</th>\n      <td>alternatives.log: update-alternatives 2024-04-...</td>\n      <td>alternatives.log</td>\n      <td>update-alternatives 2024-04-03 11:09:58: run w...</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19489</th>\n      <td>alternatives.log: update-alternatives 2024-04-...</td>\n      <td>alternatives.log</td>\n      <td>update-alternatives 2024-04-15 10:54:52: run w...</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19490</th>\n      <td>alternatives.log: update-alternatives 2024-04-...</td>\n      <td>alternatives.log</td>\n      <td>update-alternatives 2024-04-15 10:55:20: run w...</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19491</th>\n      <td>alternatives.log: update-alternatives 2024-04-...</td>\n      <td>alternatives.log</td>\n      <td>update-alternatives 2024-04-19 11:09:25: run w...</td>\n      <td>None</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>19492 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\n## Parse the log name and the content\ndf['log_name'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[0])\ndf['log_content'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[1])\n\n\n# Function to extract and standardize timestamps\ndef extract_timestamp(log_entry):\n    try:\n        return dateutil.parser.parse(log_entry.split()[0])\n    except:\n        return None\n\ndf['timestamp'] = df['log_content'].apply(extract_timestamp)\nvectorizer = TfidfVectorizer(max_features=1000) # features tested from 10, 100 and None\nX_tfidf = vectorizer.fit_transform(df['log_content']).toarray()\ndf['log_file_cat'] = df['log_name'].astype('category').cat.codes\nX_combined = np.hstack((X_tfidf, df['log_file_cat'].values.reshape(-1, 1)))\n\nsequence_length = 10  # Defining the length of each sequence\nn_features = X_combined.shape[1]  # Number of features\n\n# This function creates sequences from the dataset\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length + 1):\n        x_seq = data[i:(i + seq_length)]\n        xs.append(x_seq)\n    return np.array(xs)\n\nX_seq = create_sequences(X_combined, sequence_length)\n\nX_train, X_test = train_test_split(X_seq, test_size=0.2, random_state=42, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T13:17:53.835139Z","iopub.execute_input":"2024-05-21T13:17:53.835502Z","iopub.status.idle":"2024-05-21T13:17:56.130831Z","shell.execute_reply.started":"2024-05-21T13:17:53.835474Z","shell.execute_reply":"2024-05-21T13:17:56.130041Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def train_model(model, data, epochs):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0  # Track total loss for printing\n        for seq in data:\n            optimizer.zero_grad()\n            seq = torch.tensor(seq, dtype=torch.float32).to('cuda')\n            # seq originally [sequence_length, features]\n            # Unsqueezed seq [1, sequence_length, features]\n            output = model(seq.unsqueeze(0))\n            # output needs to match [sequence_length, features]\n            output = output.squeeze(0)  # This adjusts the output to [sequence_length, features]\n            loss = loss_function(output, seq)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        average_loss = total_loss / len(data)\n        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {average_loss}')\n\ntrain_model(model, X_train, epochs=10)","metadata":{"id":"oPjdlR9oSuYn","execution":{"iopub.status.busy":"2024-05-21T13:18:00.716449Z","iopub.execute_input":"2024-05-21T13:18:00.716799Z","iopub.status.idle":"2024-05-21T13:25:24.487848Z","shell.execute_reply.started":"2024-05-21T13:18:00.716772Z","shell.execute_reply":"2024-05-21T13:25:24.486793Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10, 1001])) that is different to the input size (torch.Size([2, 1, 1001])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Average Loss: 0.000860469332815511\nEpoch 2/10, Average Loss: 0.0008391841735260597\nEpoch 3/10, Average Loss: 0.0008091035244655652\nEpoch 4/10, Average Loss: 0.0007907099996480193\nEpoch 5/10, Average Loss: 0.0007727599794667471\nEpoch 6/10, Average Loss: 0.0007570085142316064\nEpoch 7/10, Average Loss: 0.0007467012827069061\nEpoch 8/10, Average Loss: 0.0007359471775936272\nEpoch 9/10, Average Loss: 0.0007269280846495843\nEpoch 10/10, Average Loss: 0.0007193825437610323\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\ndef read_logs_from_file(file_path):\n    with open(file_path, 'r') as file:\n        logs = file.readlines()\n    return logs\n\nfile_path = '/kaggle/input/logsanaly/UC753_final_attack.log' # !normal traffic only\nlogs = read_logs_from_file(file_path)\ndf = pd.DataFrame(logs, columns=['log_entry'])\nprint(df.shape)\n# Parse the log name and the content\ndf['log_name'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[0])\ndf['log_content'] = df['log_entry'].apply(lambda x: x.split(': ', 1)[1])\n\n\n# Function to extract and standardize timestamps\ndef extract_timestamp(log_entry):\n    try:\n        return dateutil.parser.parse(log_entry.split()[0])\n    except:\n        return None\n\ndf['timestamp'] = df['log_content'].apply(extract_timestamp)\nvectorizer = TfidfVectorizer(max_features=1000)\nX_tfidf = vectorizer.fit_transform(df['log_content']).toarray()\ndf['log_file_cat'] = df['log_name'].astype('category').cat.codes\nX_combined = np.hstack((X_tfidf, df['log_file_cat'].values.reshape(-1, 1)))\nsequence_length = 10  # Defining the length of each sequence\nn_features = X_combined.shape[1]  # Number of features\n\n# This function creates sequences from the dataset\ndef create_sequences(data, seq_length):\n    xs = []\n    for i in range(len(data) - seq_length + 1):\n        x_seq = data[i:(i + seq_length)]\n        xs.append(x_seq)\n    return np.array(xs)\n\nX_seq = create_sequences(X_combined, sequence_length)\n\n## on attack\nimport torch\n\nmodel.eval()  # Set the model to evaluation mode\ntest_errors = []\nindices = []\n\nfor i, seq in enumerate(X_seq):\n    with torch.no_grad():  # No need to compute gradients\n        seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to('cuda')  # Add batch dimension\n        reconstructed_seq = model(seq_tensor)\n        loss = torch.mean((seq_tensor - reconstructed_seq)**2).item()  # Compute mean squared error\n        test_errors.append(loss)\n\n\nthreshold = np.percentile(test_errors, 95)  # Errors above this value are considered anomalies\nprint(\"Threshold for anomalies:\", threshold)\nanomalies = [i for i, error in enumerate(test_errors) if error > threshold]\n# Manual Inspection\ndf_anomalies = df.iloc[anomalies]\n","metadata":{"id":"uXDB4PgUSuYo","execution":{"iopub.status.busy":"2024-05-21T13:25:35.390136Z","iopub.execute_input":"2024-05-21T13:25:35.390818Z","iopub.status.idle":"2024-05-21T13:25:54.413739Z","shell.execute_reply.started":"2024-05-21T13:25:35.390789Z","shell.execute_reply":"2024-05-21T13:25:54.412813Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"(19492, 1)\nThreshold for anomalies: 0.002779215644113714\n","output_type":"stream"}]},{"cell_type":"code","source":"df_anomalies['log_name'].value_counts()","metadata":{"id":"6_T6fmWJSuYo","execution":{"iopub.status.busy":"2024-05-21T13:25:54.415280Z","iopub.execute_input":"2024-05-21T13:25:54.415577Z","iopub.status.idle":"2024-05-21T13:25:54.423725Z","shell.execute_reply.started":"2024-05-21T13:25:54.415552Z","shell.execute_reply":"2024-05-21T13:25:54.422841Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"log_name\nxrdp-sesman.log               314\nubuntu-advantage.log          232\nsyslog                        232\nalternatives.log               39\nauth.log                       29\ndmesg                          28\nkern.log                       18\ndpkg.log                       18\nxrdp.log                       18\nbootstrap.log                  16\ncuda-installer.log             16\nfontconfig.log                 10\nwtmp                            3\nupgrade-policy-changed.log      2\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}